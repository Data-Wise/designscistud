---
title: 'STA305/1004 - Homework #1 Solutions'
author: "Nathan Taback"
date: "January 25, 2016"
output: pdf_document
---
# Question 1

(a)  

The following set of equations models the weighing scheme.

$$\begin{aligned}
y_1 &= \beta_1 x_{11} +\beta_2 x_{12} +\beta_3 x_{13}  +\beta_4 x_{14} + \epsilon_1 \\
y_2 &= \beta_1 x_{21} +\beta_2 x_{22} +\beta_3 x_{23}  +\beta_4 x_{24} + \epsilon_2 \\
y_3 &= \beta_1 x_{31} +\beta_2 x_{32} +\beta_3 x_{33}  +\beta_4 x_{34} + \epsilon_3 \\
y_4 &= \beta_1 x_{41} +\beta_2 x_{42} +\beta_3 x_{43}  +\beta_4 x_{44} + \epsilon_4, \\
\end{aligned}$$

where,

$$ x_{ij} = \left\{
	\begin{array}{ll}
		1  & \mbox{if the } i^{th} \mbox{ measurement of the }  j^{th} \mbox{ object is in the left pan}  \\
		-1 & \mbox{if the } i^{th} \mbox{ measurement of the }  j^{th} \mbox{ object is in right pan,}
	\end{array}
\right.$$

and $Var(\epsilon_i)=\sigma^2$.

NB: The pan that's coded as 1 or -1 is arbitrary.

In this case the design indicates ceratin values for $x_{ij}$. So that the observed measurements $y_i$ are related to the unknown weights $\beta_i$ via the four equations:

$$\begin{aligned}
y_1 &= \beta_1 +\beta_2  +\beta_3  +\beta_4  + \epsilon_1 \\
y_2 &= \beta_1  +\beta_2  -\beta_3 -\beta_4  + \epsilon_2 \\
y_3 &= \beta_1  -\beta_2  +\beta_3 - \beta_4  + \epsilon_3 \\
y_4 &= \beta_1  -\beta_2 - \beta_3  +\beta_4  + \epsilon_4. \\
\end{aligned}$$

In matrix form this could be written as $y=X\beta+\epsilon$, where

$$y=(y_1,y_2,y_3,y_4)^{\prime},\thinspace X=
 \begin{pmatrix}
  1 & 1 & 1 & 1 \\
  1 & 1 & -1 & -1 \\
  1 & -1 & 1 & -1 \\
  1 & -1 & -1 & 1
 \end{pmatrix}, \\ \thinspace {\beta}=(\beta_1,\beta_2,\beta_3,\beta_4)^{\prime}, \epsilon=(\epsilon_1,\epsilon_2,\epsilon_3,\epsilon_4)^{\prime}.$$

(b)  The least squares estimates can be found using ${\hat \beta}=\left(X^{T} X\right)^{-1}X^{T}y.$  Using R we find ${\hat \beta}=\left(X^{T} X\right)^{-1}X^{T}$ then multiply it by $y$.

```{r,comment=""}
X <-rbind(c(1,1,1,1),
          c(1,1,-1,-1),
          c(1,-1,1,-1),
          c(1,-1,-1,1))
solve( t(X) %*% X ) %*% t(X)
```

So
$$\begin{aligned}
\hat{\beta}_1 &= (1/4) (y_1+y_2+y_3+y_4) \\
\hat{\beta}_2 &= (1/4) (y_1+y_2-y_3-y_4) \\
\hat{\beta}_3 &= (1/4) (y_1-y_2+y_3-y_4) \\
\hat{\beta}_4 &= (1/4) (y_1-y_2-y_3+y_4). \\
\end{aligned}$$


(c)  The standard error of $\hat \beta$ is the square-root of the diagnal entries of the covariance matrix of  $\hat \beta$, namely, $\left(X^{T} X\right)^{-1}{\sigma}^2$. The covariance matrix can be found using R.

```{r,comment=""}
X <-rbind(c(1,1,1,1),
          c(1,1,-1,-1),
          c(1,-1,1,-1),
          c(1,-1,-1,1))
solve( t(X) %*% X )
```

The $se\left(\hat \beta_i \right)=\sigma/2, \thinspace i=1,...,4$.

(d)  If she measured each object twice then the precision for each object would be the standard deviation of the average of the two measurements of the object.  Since each has precision $\sigma$ the standard deviation of the average is $\sigma/2$. So, the precision is the same compared to the proposed design using a pan balance.  Although, weighing each object twice requires eight measurements and the pan balance design requires four measurements.  

(e) In this design 

$$\begin{aligned}
y_1 &= \beta_1 x_{11} +\beta_2 x_{12} +\beta_3 x_{13}  +\beta_4 x_{14} + \epsilon_1 \\
y_2 &= \beta_1 x_{21} +\beta_2 x_{22} +\beta_3 x_{23}  +\beta_4 x_{24} + \epsilon_2 \\
y_3 &= \beta_1 x_{31} +\beta_2 x_{32} +\beta_3 x_{33}  +\beta_4 x_{34} + \epsilon_3 \\
y_4 &= \beta_1 x_{41} +\beta_2 x_{42} +\beta_3 x_{43}  +\beta_4 x_{44} + \epsilon_4, \\
\end{aligned}$$

where,

$$ x_{ij} = \left\{
	\begin{array}{ll}
		1  & \mbox{if the } i^{th} \mbox{ measurement of the }  j^{th} \mbox{ object is weighed}  \\
		0 & \mbox{if the } i^{th} \mbox{ measurement of the }  j^{th} \mbox{ object is not weighed}
	\end{array}
\right.$$

So,

$$\begin{aligned}
y_1 &= \beta_1 +\beta_2  +\beta_3  +\beta_4  + \epsilon_1 \\
y_2 &= \beta_1  +\beta_2   + \epsilon_2 \\
y_3 &= \beta_1    +\beta_3   + \epsilon_3 \\
y_4 &= \beta_1    +\beta_4  + \epsilon_4. \\
\end{aligned}$$

The least squares estimates are obtained by multiplying $\left(X^{T} X\right)^{-1}X^{T}$ by $y$.

```{r,comment=""}
X1 <-rbind(c(1,1,1,1),
           c(1,1,0,0),
           c(1,0,1,0),
           c(1,0,0,1))
solve( t(X1) %*% X1 ) %*% t(X1)
```

So,

$$\begin{aligned}
\hat{\beta}_1 &= (1/2) (-y_1+y_2+y_3+y_4) \\
\hat{\beta}_2 &= (1/2) (y_1+y_2-y_3-y_4) \\
\hat{\beta}_3 &= (1/2) (y_1-y_2+y_3-y_4) \\
\hat{\beta}_4 &= (1/2) (y_1-y_2-y_3+y_4). \\
\end{aligned}$$

As in the previous part use R to calculate the standard error of $\hat \beta$, by taking the square-root of the diagonal elements of $\left(X^{T} X\right)^{-1}$ and multiplying these values by $\sigma/2$. The covariance matrix can be found using R.

```{r,comment=""}
X1 <-rbind(c(1,1,1,1),
           c(1,1,0,0),
           c(1,0,1,0),
           c(1,0,0,1))
solve( t(X1) %*% X1 )
```

This design achieves the same precision as the original pan balance design using the same number of measurements.  This is because the precison of each measurement is greater using the digital versus pan balance scale.

# Question 2. 

(a) There are ${15 \choose 8}={15 \choose 7}=$ `r choose(15,8)`.  Eight judges will be randomly assigned to brand A then the remaining seven will be assigned to brand B or vice versa.

(b), (c)  The following R code calculates the randomization distribution and a two-sided p-value for testing $H_0: \mu_A=\mu_b$, where $\mu_A, \mu_B$ are the mean taste rating for brands A and B respectively.  The p-value is two-sided since the question doesn't state a hypotheis of A better than B or B better than A.  Also, part (c) asks if there is evidence of a "significant difference" which implies a two-sided test.  

```{r,cache=TRUE,comment="",message=FALSE}
yA <- c(2,4,2,1,9,9,2,2)
yB <- c(8,3,5,3,7,7,4)
beer <- c(yA,yB) #pool data
N <- choose(15,8)
res <- numeric(N) # store the results
#install.packages("combinat") # if package not installed then remove comment
library(combinat)
index <-combn(1:15,8) # Generate N treatment assignments
for (i in 1:N)
{
  res[i] <- mean(beer[index[,i]])-mean(beer[-index[,i]])
}
hist(res,xlab="ybarA-ybarB", main="Randomization Distribution of difference in means")

observed <- mean(yA)-mean(yB) #store observed mean difference
abline(v=observed,col="blue") #add line at observed mean diff
abline(v=-observed,col="blue") #add line at observed mean diff

tbar <- mean(res)
abline(v=tbar,col="red") #add line at observed mean diff
pval <- sum(abs(res-tbar)>=abs(observed-tbar))/N
pval
```


The p-value of the test is `r pval`.  Therefore there is no evidence to suggest that there is a difference between the two brands.

# Question 3. 

(a)  R indicates right eye and L indicates left eye.

$$\begin{array}{c |c c c }
Subject & Drug A & Drug B & Toss\\
\hline
1 & 7 (R) & 9 (L) & T\\
2 & 3 (L) & 5 (R) & H\\
3 & 8 (L) & 12 (R)  & H\\
4 & 11 (L) & 4 (R)  & H\\
5 & 4 (L) & 6 (R) & H 
\end{array}$$

(b)  Randomized paired design since each subject's eyes were randomized as a pair.

(c) Probability is 1/2 since the coin is fair.

(d)  The probability of any particular treatment allocation is $\frac{1}{2^5}$. The probability of obtaining the treatment allocation of THHHH is the same as TTTHH or HHHTT, etc. The $2^5$ treatment assignments can be seen using the following R code.

```{r,comment=""}
LR <- list(c("T","H")) # difference is multiplied by -1 or 1
trtassign <- expand.grid(rep(LR, 5))
trtassign
```

The researcher in this study used the $31^{st}$ treatment assignment in the list.  But there was an equal chance that the first assignment could have been used.  

(e)  Let $\mu_{\bar d}$ be the mean of the average paired difference.  The null hypothesis is $H_0:\mu_d=0$.  The randomization distribution is calculated by assuming that $H_0$ is true and the two results obtained from each particular are exchangeable.  There are $2^5=32$ arrangements of signs in the average difference.

```{r,comment=""}
drugA <- c(7,3,8,11,4)
drugB <- c(9,5,12,4,6)
diff <- drugA-drugB
meandiff <- mean(diff)
N <- 2^(5) # number of treatment assignments
res <- numeric(N) #vector to store results
LR <- list(c(-1,1)) # difference is multiplied by -1 or 1
trtassign <- expand.grid(rep(LR, 5)) # generate all possible treatment assign

for(i in 1:N){
res[i] <- mean(as.numeric(trtassign[i,])*diff)
}
res # all possible mean differences
hist(res, main="Randomization Distribution of mean difference")
```

This is the same as calculating ${\bar d}$ for each of the 32 arrangements of signs in:

$$ {\bar d} = \frac{\pm 2 \pm 2 \pm 4 \pm 7 \pm 2}{5}.$$

(f)  This is a two-sided test since the question asks for "evidence of a difference".  The two-sided p-value is obtained using the following R code.

```{r,comment="", cache=TRUE}
drugA <- c(7,3,8,11,4)
drugB <- c(9,5,12,4,6)
diff <- drugA-drugB
meandiff <- mean(diff)
N <- 2^(5) # number of treatment assignments
res <- numeric(N) #vector to store results
LR <- list(c(-1,1)) # difference is multiplied by -1 or 1
trtassign <- expand.grid(rep(LR, 5)) # generate all possible treatment assign

for(i in 1:N){
res[i] <- mean(as.numeric(trtassign[i,])*diff)
}
tbar <- mean(res)
pval <- sum(abs(res-tbar)>=abs(meandiff-tbar))/N
pval
```

The p-value is `r pval`.  There is no evidence to indicate that the two drugs have different log intraoccular pressures.

# Question 4

(a)  This is a paired t-test.

```{r,comment=""}
t.test(drugA,drugB,paired = T)
qqnorm(diff);qqline(diff)
```

The two assumptions are: (i) independence; and (ii) normality.  The subjects are differences are independent since they don't depend on other subjects differences.  There are only five observations so it's difficult to assess normaility.  Nevertheless, the normal quantile plot does not suggest that this data are normally distributed.

(b)  The p-values from the randomization test and paired t-test agree.  The t-test is known to be robust against non-normality.  The randomization test does not assume that the differences are normally distributed, yet the p-value is the same (to the first decimal place) as this test.  This is most likely an example of the robustness of t-test in action.    
